[
  {
    "objectID": "CONTRIBUTING.html",
    "href": "CONTRIBUTING.html",
    "title": "How to contribute",
    "section": "",
    "text": "Before anything else, please install the git hooks that run automatic scripts during each commit and merge to strip the notebooks of superfluous metadata (and avoid merge conflicts). After cloning the repository, run the following command inside it:\nnbdev_install_git_hooks\n\n\n\n\nEnsure the bug was not already reported by searching on GitHub under Issues.\nIf you’re unable to find an open issue addressing the problem, open a new one. Be sure to include a title and clear description, as much relevant information as possible, and a code sample or an executable test case demonstrating the expected behavior that is not occurring.\nBe sure to add the complete error messages.\n\n\n\n\nOpen a new GitHub pull request with the patch.\nEnsure that your PR includes a test that fails without your patch, and pass with it.\nEnsure the PR description clearly describes the problem and solution. Include the relevant issue number if applicable.\n\n\n\n\n\n\nKeep each PR focused. While it’s more convenient, do not combine several unrelated fixes together. Create as many branches as needing to keep each PR focused.\nDo not mix style changes/fixes with “functional” changes. It’s very difficult to review such PRs and it most likely get rejected.\nDo not add/remove vertical whitespace. Preserve the original style of the file you edit as much as you can.\nDo not turn an already submitted PR into your development playground. If after you submitted PR, you discovered that more work is needed - close the PR, do the required work and then submit a new PR. Otherwise each of your commits requires attention from maintainers of the project.\nIf, however, you submitted a PR and received a request for changes, you should proceed with commits inside that PR, so that the maintainer can see the incremental fixes and won’t need to review the whole PR again. In the exception case where you realize it’ll take many many commits to complete the requests, then it’s probably best to close the PR, do the work and then submit it again. Use common sense where you’d choose one way over another.\n\n\n\n\n\nDocs are automatically created from the notebooks in the nbs folder."
  },
  {
    "objectID": "CONTRIBUTING.html#how-to-get-started",
    "href": "CONTRIBUTING.html#how-to-get-started",
    "title": "How to contribute",
    "section": "",
    "text": "Before anything else, please install the git hooks that run automatic scripts during each commit and merge to strip the notebooks of superfluous metadata (and avoid merge conflicts). After cloning the repository, run the following command inside it:\nnbdev_install_git_hooks"
  },
  {
    "objectID": "CONTRIBUTING.html#did-you-find-a-bug",
    "href": "CONTRIBUTING.html#did-you-find-a-bug",
    "title": "How to contribute",
    "section": "",
    "text": "Ensure the bug was not already reported by searching on GitHub under Issues.\nIf you’re unable to find an open issue addressing the problem, open a new one. Be sure to include a title and clear description, as much relevant information as possible, and a code sample or an executable test case demonstrating the expected behavior that is not occurring.\nBe sure to add the complete error messages.\n\n\n\n\nOpen a new GitHub pull request with the patch.\nEnsure that your PR includes a test that fails without your patch, and pass with it.\nEnsure the PR description clearly describes the problem and solution. Include the relevant issue number if applicable."
  },
  {
    "objectID": "CONTRIBUTING.html#pr-submission-guidelines",
    "href": "CONTRIBUTING.html#pr-submission-guidelines",
    "title": "How to contribute",
    "section": "",
    "text": "Keep each PR focused. While it’s more convenient, do not combine several unrelated fixes together. Create as many branches as needing to keep each PR focused.\nDo not mix style changes/fixes with “functional” changes. It’s very difficult to review such PRs and it most likely get rejected.\nDo not add/remove vertical whitespace. Preserve the original style of the file you edit as much as you can.\nDo not turn an already submitted PR into your development playground. If after you submitted PR, you discovered that more work is needed - close the PR, do the required work and then submit a new PR. Otherwise each of your commits requires attention from maintainers of the project.\nIf, however, you submitted a PR and received a request for changes, you should proceed with commits inside that PR, so that the maintainer can see the incremental fixes and won’t need to review the whole PR again. In the exception case where you realize it’ll take many many commits to complete the requests, then it’s probably best to close the PR, do the work and then submit it again. Use common sense where you’d choose one way over another."
  },
  {
    "objectID": "CONTRIBUTING.html#do-you-want-to-contribute-to-the-documentation",
    "href": "CONTRIBUTING.html#do-you-want-to-contribute-to-the-documentation",
    "title": "How to contribute",
    "section": "",
    "text": "Docs are automatically created from the notebooks in the nbs folder."
  },
  {
    "objectID": "feature.html",
    "href": "feature.html",
    "title": "Feature",
    "section": "",
    "text": "source\n\nto_np\n\n to_np (array:Union[List,Tuple,numpy.ndarray,torch.Tensor])\n\n\nsource\n\n\nto_torch\n\n to_torch (x:Union[List,&lt;built-infunctionarray&gt;,torch.Tensor])\n\n\nsource\n\n\nto_numpy_image\n\n to_numpy_image (img:Union[str,&lt;built-infunctionarray&gt;,torch.Tensor])\n\n\nassert isinstance(to_numpy_image('data/strahov.png'), np.ndarray)\n\nlibpng warning: iCCP: known incorrect sRGB profile\n\n\n\nsource\n\n\nopencv_kpts_from_laf\n\n opencv_kpts_from_laf (lafs:torch.Tensor, mrSize:float=1.0,\n                       resps:Optional[torch.Tensor]=None)\n\n\nsource\n\n\nlaf_from_opencv_kpts\n\n laf_from_opencv_kpts (kpts:List[cv2.KeyPoint], mrSize:float=6.0,\n                       device:torch.device=device(type='cpu'),\n                       with_resp:bool=False)\n\nLet’s detect ORB keypoints and convert them to and from OpenCV\n\nimg = cv2.cvtColor(cv2.imread('data/strahov.png'), cv2.COLOR_BGR2RGB)\n\ndet = cv2.ORB_create(500)\nkps, descs = det.detectAndCompute(img, None)\n\nout_img = cv2.drawKeypoints(img, kps, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\nplt.imshow(out_img)\n\nlibpng warning: iCCP: known incorrect sRGB profile\n\n\n&lt;matplotlib.image.AxesImage&gt;\n\n\n\n\n\n\nimg = cv2.cvtColor(cv2.imread('data/strahov.png'), cv2.COLOR_BGR2RGB)\n\ndet = cv2.ORB_create(500)\nkps, descs = det.detectAndCompute(img, None)\nlafs, r = laf_from_opencv_kpts(kps, 1.0, with_resp=True)\n\n\nimg = cv2.cvtColor(cv2.imread('data/strahov.png'), cv2.COLOR_BGR2RGB)\n\ndet = cv2.ORB_create(500)\nkps, descs = det.detectAndCompute(img, None)\nlafs, r = laf_from_opencv_kpts(kps, 1.0, with_resp=True)\nkps_back = opencv_kpts_from_laf(lafs, 1.0, r)\nout_img = cv2.drawKeypoints(img, kps_back, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\nplt.imshow(out_img)\n\n&lt;matplotlib.image.AxesImage&gt;\n\n\n\n\n\nOpenCV uses different conventions for the local feature scale.\nE.g. to get equivalent kornia LAF from ORB keypoints, one should you mrSize = 0.5, while for SIFT – 6.0. The orientation convention is also different for kornia and OpenCV.\n\nsource\n\n\nopencv_ORB_kpts_from_laf\n\n opencv_ORB_kpts_from_laf (lafs, resps:Optional[torch.Tensor]=None)\n\n\nsource\n\n\nopencv_SIFT_kpts_from_laf\n\n opencv_SIFT_kpts_from_laf (lafs, resps:Optional[torch.Tensor]=None)\n\n\nsource\n\n\nlaf_from_opencv_SIFT_kpts\n\n laf_from_opencv_SIFT_kpts (kpts:List[cv2.KeyPoint],\n                            device:torch.device=device(type='cpu'),\n                            with_resp:bool=False)\n\n\nsource\n\n\nlaf_from_opencv_ORB_kpts\n\n laf_from_opencv_ORB_kpts (kpts:List[cv2.KeyPoint],\n                           device:torch.device=device(type='cpu'),\n                           with_resp:bool=False)\n\n\nimg = cv2.cvtColor(cv2.imread('data/strahov.png'), cv2.COLOR_BGR2RGB)\n\ndet = cv2.SIFT_create(500)\nkps, descs = det.detectAndCompute(img, None)\n\nout_img = cv2.drawKeypoints(img, kps, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\nplt.imshow(out_img)\n\nlibpng warning: iCCP: known incorrect sRGB profile\n\n\n&lt;matplotlib.image.AxesImage&gt;\n\n\n\n\n\nThe keypoints are small, because, unlike for ORB, for SIFT OpenCV draws not real regions to be described, but the radius of the blobs, which are detected. Kornia and kornia_moons, inlike OpenCV, shows the real description region.\n\nlafs, r = laf_from_opencv_SIFT_kpts(kps, with_resp=True)\nvisualize_LAF(image_to_tensor(img, False), lafs, 0, 'y', figsize=(8,6))\n\n\n\n\n\n\n\nIf you want to see the image, similar to OpenCV one, you can scale LAFs by factor 1/12.\n\nvisualize_LAF(image_to_tensor(img, False),\n              kornia.feature.laf.scale_laf(lafs, 1./6.0), 0, 'y', figsize=(8,6))\n\n\n\n\n\n\n\nNow let’s do the same for matches format\n\nsource\n\n\nkornia_matches_from_cv2\n\n kornia_matches_from_cv2 (cv2_matches, device=device(type='cpu'))\n\n\nsource\n\n\ncv2_matches_from_kornia\n\n cv2_matches_from_kornia (match_dists:torch.Tensor,\n                          match_idxs:torch.Tensor)\n\n\nimg = cv2.cvtColor(cv2.imread('data/strahov.png'), cv2.COLOR_BGR2RGB)\n\ndet = cv2.SIFT_create(500)\nkps, descs = det.detectAndCompute(img, None)\n                                  \nmatch_dists, match_idxs = kornia.feature.match_nn(torch.from_numpy(descs).float(),\n                                             torch.from_numpy(descs).float())\n\ncv2_matches = cv2_matches_from_kornia(match_dists, match_idxs)\nout_img = cv2.drawMatches(img, kps, img, kps, cv2_matches, None,\n                          flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\nplt.figure(figsize=(10,5))\nplt.imshow(out_img)\n\nmatch_dists_back, match_idxs_back = kornia_matches_from_cv2(cv2_matches)\n\nassert(allclose(match_dists_back, match_dists))\nassert(allclose(match_idxs_back, match_idxs))\n\nlibpng warning: iCCP: known incorrect sRGB profile\n\n\n\n\n\n\nsource\n\n\nOpenCVDetectorWithAffNetKornia\n\n OpenCVDetectorWithAffNetKornia (opencv_detector, make_upright=False,\n                                 mrSize:float=6.0, max_kpts=-1)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\nOpenCVFeatureKornia\n\n OpenCVFeatureKornia (opencv_detector, mrSize:float=6.0)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\nOpenCVDetectorKornia\n\n OpenCVDetectorKornia (opencv_detector, mrSize:float=6.0,\n                       make_upright=False, max_kpts=-1)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\nmake_keypoints_upright\n\n make_keypoints_upright (kpts)\n\n\nkornia_cv2dog = OpenCVDetectorKornia(cv2.SIFT_create(500))\nkornia_cv2sift = OpenCVFeatureKornia(cv2.SIFT_create(500))\n\n\ntimg = image_to_tensor(cv2.cvtColor(cv2.imread('data/strahov.png'), cv2.COLOR_BGR2RGB), False).float()/255.\n\n\nlafs, r = kornia_cv2dog(timg)\nlafs2, r2, descs2 = kornia_cv2sift(timg)\n\n\nvisualize_LAF(timg, lafs, 0, 'y', figsize=(8,6))\n\nlibpng warning: iCCP: known incorrect sRGB profile\n\n\n\n\n\n\n\n\n\nkornia_cv2dogaffnet = OpenCVDetectorWithAffNetKornia(cv2.SIFT_create(500), make_upright=True)\n\n\ntimg = image_to_tensor(cv2.cvtColor(cv2.imread('data/strahov.png'), cv2.COLOR_BGR2RGB), False).float()/255.\n\n\nlafs, r = kornia_cv2dogaffnet(timg)\n\n\nvisualize_LAF(timg, lafs, 0, 'y', figsize=(8,6))\n\n/var/folders/j9/y_61c9h10xz3d5g4d1rrny5c0000gn/T/ipykernel_19919/2708263890.py:65: DeprecationWarning: `LAFAffNetShapeEstimator` default behaviour is changed and now it does preserve original LAF orientation. Make sure your code accounts for this.\n  self.affnet = kornia.feature.LAFAffNetShapeEstimator(True).eval()\nlibpng warning: iCCP: known incorrect sRGB profile"
  },
  {
    "objectID": "CHANGELOG.html",
    "href": "CHANGELOG.html",
    "title": "Release notes",
    "section": "",
    "text": "Release notes\nFix the torch-numpy conversion bug\nRelease with nbdev 2.0, quatro docs ## 0.2.7"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kornia_moons",
    "section": "",
    "text": "pip install kornia_moons"
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "Kornia_moons",
    "section": "",
    "text": "pip install kornia_moons"
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "Kornia_moons",
    "section": "How to use",
    "text": "How to use\nHere is an example how to use kornia-moons for local feature conversion from OpenCV ORB keypoints\n\nimport matplotlib.pyplot as plt\nimport cv2\nimport torch\nimport kornia as K\nfrom typing import List\nimport matplotlib.pyplot as plt\n\nfrom kornia_moons.feature import laf_from_opencv_ORB_kpts, opencv_ORB_kpts_from_laf \nfrom kornia_moons.viz import visualize_LAF\n\n\n\nimg = cv2.cvtColor(cv2.imread('data/strahov.png'), cv2.COLOR_BGR2RGB)\n\ndet = cv2.ORB_create(500)\nkps, descs = det.detectAndCompute(img, None)\n\nout_img = cv2.drawKeypoints(img, kps, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\nplt.imshow(out_img)\n\n\nlafs = laf_from_opencv_ORB_kpts(kps)\nvisualize_LAF(K.image_to_tensor(img, False), lafs, 0)\n\nkps_back = opencv_ORB_kpts_from_laf(lafs)\nout_img2 = cv2.drawKeypoints(img, kps_back, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\nplt.imshow(out_img2)\n\nlibpng warning: iCCP: known incorrect sRGB profile\n\n\n\n\n\n\n\n\n&lt;matplotlib.image.AxesImage&gt;"
  },
  {
    "objectID": "viz.html",
    "href": "viz.html",
    "title": "Viz",
    "section": "",
    "text": "from kornia_moons.feature import (\n     laf_from_opencv_kpts,\n     opencv_kpts_from_laf,\n     laf_from_opencv_ORB_kpts,\n    laf_from_opencv_SIFT_kpts)\n\n\nsource\n\nvisualize_LAF\n\n visualize_LAF (img, LAF, img_idx=0, color='r', linewidth=1,\n                draw_ori=True, fig=None, ax=None, return_fig_ax=False,\n                **kwargs)\n\nLet’s detect ORB keypoints and convert them to and from OpenCV\n\nimg = cv2.cvtColor(cv2.imread('data/strahov.png'), cv2.COLOR_BGR2RGB)\n\ndet = cv2.ORB_create(500)\nkps, descs = det.detectAndCompute(img, None)\n\nout_img = cv2.drawKeypoints(img, kps, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\nplt.imshow(out_img)\n\n&lt;matplotlib.image.AxesImage&gt;\n\n\n\n\n\n\nimg = cv2.cvtColor(cv2.imread('data/strahov.png'), cv2.COLOR_BGR2RGB)\n\ndet = cv2.ORB_create(500)\nkps, descs = det.detectAndCompute(img, None)\nlafs, r = laf_from_opencv_kpts(kps, 1.0, with_resp=True)\nfig=plt.figure()\nvisualize_LAF(image_to_tensor(img, False), lafs, 0, 'y', draw_ori=False,figsize=(8,6), linewidth=2)\n\n&lt;Figure size 432x288 with 0 Axes&gt;\n\n\n\n\n\n\nimg = cv2.cvtColor(cv2.imread('data/strahov.png'), cv2.COLOR_BGR2RGB)\n\ndet = cv2.ORB_create(500)\nkps, descs = det.detectAndCompute(img, None)\nlafs, r = laf_from_opencv_kpts(kps, 1.0, with_resp=True)\nkps_back = opencv_kpts_from_laf(lafs, 1.0, r)\nout_img = cv2.drawKeypoints(img, kps_back, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\nplt.imshow(out_img)\n\n&lt;matplotlib.image.AxesImage&gt;\n\n\n\n\n\nOpenCV uses different conventions for the local feature scale.\nE.g. to get equivalent kornia LAF from ORB keypoints, one should you mrSize = 0.5, while for SIFT – 6.0. The orientation convention is also different for kornia and OpenCV.\n\nsource\n\n\nepilines_to_start_end_points\n\n epilines_to_start_end_points (epi, h, w)\n\n\nsource\n\n\ndraw_LAF_matches\n\n draw_LAF_matches (lafs1, lafs2, tent_idxs, img1, img2, inlier_mask=None,\n                   draw_dict={'inlier_color': (0.2, 1, 0.2),\n                   'tentative_color': (0.8, 0.8, 0), 'feature_color':\n                   (0.2, 0.5, 1), 'vertical': False}, Fm:Optional[&lt;built-\n                   infunctionarray&gt;]=None, H:Optional[&lt;built-\n                   infunctionarray&gt;]=None, fig=None, ax:Optional=None,\n                   return_fig_ax=False)\n\nThis function draws LAFs, tentative matches, inliers epipolar lines (if F is provided), and image1 corners reprojection into image 2 (if H is provided)\nWe will visualize ORB features (blue), tentative matches (yellow) and inliers(greenish)\n\nimport numpy as np\ndet = cv2.ORB_create(100)\nimg1_fname = 'data/strahov.png'\nkps1, descs1 = det.detectAndCompute(cv2.imread(img1_fname,0), None)\nlafs1 = laf_from_opencv_ORB_kpts(kps1)\nidxs = torch.stack([torch.arange(50),torch.arange(50)], dim=-1)\nfig, ax = draw_LAF_matches(lafs1, lafs1, idxs,\n                  img1_fname,img1_fname, \n                  [True if i%2 == 0 else False for i in range(len(idxs))],\n                    draw_dict={\"inlier_color\": (0.2, 1, 0.2),\n                               \"tentative_color\": (0.8, 0.8, 0), \n                               \"feature_color\": (0.2, 0.5, 1),\n                              \"vertical\": False}, return_fig_ax=True)\n\n\n\n\nNow let’s try with epipolar matrix for the translation. Inliers should lie on the horizontal epipolar lines\n\nFmat = np.array([[0., 0., 0.],\n                 [0, 0, -1],\n                [0, 1, 0]])\ndraw_LAF_matches(lafs1, lafs1, idxs,\n                  img1_fname,img1_fname, \n                  [True if i%2 == 0 else False for i in range(len(idxs))],\n                    draw_dict={\"inlier_color\": (0.2, 1, 0.2),\n                               \"tentative_color\": (0.8, 0.8, 0), \n                               \"feature_color\": (0.2, 0.5, 1),\n                              \"vertical\": True}, Fm = Fmat)\n\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\n\n\n\n\n\nNow we will transform the image, match it, find the homography and visualize it.\n\nimport numpy as np\ndet = cv2.SIFT_create(100)\nimg1_fname = 'data/strahov.png'\nimg1 = cv2.cvtColor(cv2.imread(img1_fname), cv2.COLOR_BGR2RGB)\n\nHgt = np.array([[0.5, 0.1, 10],\n                [-0.1, 0.5, 10],\n               [0, 0, 1]])\nimg2 = cv2.warpPerspective(img1, Hgt, img1.shape[:2][::-1], borderValue=(255,255,255))\n\n\n\nkps1, descs1 = det.detectAndCompute(img1, None)\nlafs1 = laf_from_opencv_SIFT_kpts(kps1)\n\nkps2, descs2 = det.detectAndCompute(img2, None)\nlafs2 = laf_from_opencv_SIFT_kpts(kps2)\n\n\nmatch_dists, match_idxs = kornia.feature.match_snn(torch.from_numpy(descs1).float(),\n                                              torch.from_numpy(descs2).float(), 0.98)\n\nH, mask = cv2.findHomography(kornia.feature.get_laf_center(lafs1[:,match_idxs[:,0]]).detach().cpu().numpy().reshape(-1,2),\n                             kornia.feature.get_laf_center(lafs2[:,match_idxs[:,1]]).detach().cpu().numpy().reshape(-1,2),\n                             cv2.USAC_MAGSAC, 0.5)\n\n                             \ndraw_LAF_matches(lafs1, lafs2, match_idxs,\n                  img1, img2, \n                  mask,\n                  draw_dict={\"inlier_color\": (0.2, 1, 0.2),\n                               \"tentative_color\": (0.8, 0.8, 0), \n                               \"feature_color\": None,\n                              \"vertical\": False}, H = H)\n\n\n\n\nAnd the same with fundamental matrix\n\nimport numpy as np\ndet = cv2.SIFT_create(75)\nimg1_fname = 'data/strahov.png'\nimg1 = cv2.cvtColor(cv2.imread(img1_fname), cv2.COLOR_BGR2RGB)\n\nHgt = np.array([[0.75, -0.1, 10],\n                [0.1, 0.75, 10],\n               [0, 0, 1]])\n\nimg2 = cv2.warpPerspective(img1, Hgt, img1.shape[:2][::-1], borderValue=(255,255,255))\n\n\nkps1, descs1 = det.detectAndCompute(img1, None)\nlafs1 = laf_from_opencv_SIFT_kpts(kps1)\n\nkps2, descs2 = det.detectAndCompute(img2, None)\nlafs2 = laf_from_opencv_SIFT_kpts(kps2)\n\n\n\nmatch_dists, match_idxs = kornia.feature.match_snn(torch.from_numpy(descs1).float(),\n                                              torch.from_numpy(descs2).float(), 0.95)\n\nFmat, mask = cv2.findFundamentalMat(kornia.feature.get_laf_center(lafs1[:,match_idxs[:,0]]).detach().cpu().numpy().reshape(-1,2),\n                             kornia.feature.get_laf_center(lafs2[:,match_idxs[:,1]]).detach().cpu().numpy().reshape(-1,2),\n                             cv2.USAC_MAGSAC, 0.5)\n\n                             \ndraw_LAF_matches(lafs1, lafs2, match_idxs,\n                  img1, img2, \n                  mask,\n                  draw_dict={\"inlier_color\": None,#(0.2, 1, 0.2),\n                               \"tentative_color\": (0.8, 0.8, 0), \n                               \"feature_color\": None,\n                              \"vertical\": True})\n\n\n\n\n\nsource\n\n\ndraw_LAF_matches_from_result_dict\n\n draw_LAF_matches_from_result_dict (result_dict, img1, img2,\n                                    draw_dict={'inlier_color': (0.2, 1,\n                                    0.2), 'tentative_color': (0.8, 0.8,\n                                    0), 'feature_color': (0.2, 0.5, 1),\n                                    'vertical': False})\n\n\nsource\n\n\ndraw_LAF_inliers_perspective_repjojected\n\n draw_LAF_inliers_perspective_repjojected (lafs1, lafs2, tent_idxs, img1,\n                                           img2, inlier_mask=None,\n                                           draw_dict={'inlier_color':\n                                           (0.2, 1, 0.2),\n                                           'reprojected_color': (0.2, 0.5,\n                                           1), 'vertical': False},\n                                           H:&lt;built-infunctionarray&gt;=None,\n                                           fig=None, ax:Optional=None,\n                                           return_fig_ax=False)\n\nThis function draws tentative matches and inliers given the homography H\n\ndet = cv2.SIFT_create(500)\nimg1_fname = 'data/img1.ppm'\nimg2_fname = 'data/img4.ppm'\n\n\nimg1 = cv2.cvtColor(cv2.imread(img1_fname), cv2.COLOR_BGR2RGB)\nimg2 = cv2.cvtColor(cv2.imread(img2_fname), cv2.COLOR_BGR2RGB)\n\n\n\nHgt = np.loadtxt('data/H1to4p')\nimg2 = cv2.warpPerspective(img1, Hgt, img1.shape[:2][::-1], borderValue=(255,255,255))\n\n\n\nkps1, descs1 = det.detectAndCompute(img1, None)\nlafs1 = laf_from_opencv_SIFT_kpts(kps1)\nkps2, descs2 = det.detectAndCompute(img2, None)\nlafs2 = laf_from_opencv_SIFT_kpts(kps2)\n\n\nmatch_dists, match_idxs = kornia.feature.match_snn(torch.from_numpy(descs1).float(),\n                                              torch.from_numpy(descs2).float(), 0.98)\n\nH, mask = cv2.findHomography(kornia.feature.get_laf_center(lafs1[:,match_idxs[:,0]]).detach().cpu().numpy().reshape(-1,2),\n                             kornia.feature.get_laf_center(lafs2[:,match_idxs[:,1]]).detach().cpu().numpy().reshape(-1,2),\n                             cv2.USAC_MAGSAC, 0.5)\n\n                             \ndraw_LAF_inliers_perspective_repjojected(lafs1, lafs2, match_idxs,\n                  cv2.cvtColor(cv2.cvtColor(img1,cv2.COLOR_RGB2GRAY), cv2.COLOR_GRAY2RGB),\n                  cv2.cvtColor(cv2.cvtColor(img2,cv2.COLOR_RGB2GRAY), cv2.COLOR_GRAY2RGB),\n                  mask, H = H)\n\n\n\n\n\nsource\n\n\ndraw_epipolar_errors_in_single_image\n\n draw_epipolar_errors_in_single_image (kp1:&lt;built-infunctionarray&gt;,\n                                       kp2:&lt;built-infunctionarray&gt;,\n                                       Fm1to2:&lt;built-infunctionarray&gt;,\n                                       img, draw_dict={'error_color': (1,\n                                       0.2, 0.2), 'feature_color': (0.2,\n                                       0.5, 1), 'figsize': (10, 10),\n                                       'markersize': 8}, img_index:int=2,\n                                       ax:Optional=None, title=None)\n\nThis function draws epipolar errors in single image\n\nkps1 = np.random.randint(0, 450, (10, 2)).astype(np.float32)\nkps2 = kps1 + np.array([20., 20.]).astype(np.float32)\n\ntimg = image_to_tensor(cv2.cvtColor(cv2.imread('data/strahov.png'), cv2.COLOR_BGR2RGB), False).float()/255.\nFm = torch.tensor([[0, 0, 0],\n                  [0, 0, 1],\n                  [0, -1, 0.]]).float()\n\nax = draw_epipolar_errors_in_single_image(kps1, kps2, Fm, timg)\n\n\n\n\n\nsource\n\n\nplot_color_line_matches\n\n plot_color_line_matches (lines, lw=2, indices=(0, 1))\n\nPlot line matches for existing images with multiple colors. Args: lines: list of ndarrays of size (N, 2, 2). order: […, 0] is y, […, 0] is x lw: line width as float pixels. indices: indices of the images to draw the matches on.\n\nsource\n\n\nplot_lines\n\n plot_lines (lines, line_colors='orange', point_colors='cyan', ps=4, lw=2,\n             indices=(0, 1))\n\nPlot lines and endpoints for existing images. Args: lines: list of ndarrays of size (N, 2, 2). order: […, 0] is y, […, 0] is x colors: string, or list of list of tuples (one for each keypoints). ps: size of the keypoints as float pixels. lw: line width as float pixels. indices: indices of the images to draw the matches on.\n\nsource\n\n\nplot_images\n\n plot_images (imgs, titles=None, cmaps='gray', dpi=100, size=6, pad=0.5)\n\nPlot a set of images horizontally. Args: imgs: a list of NumPy or PyTorch images, RGB (H, W, 3) or mono (H, W). titles: a list of strings, as titles for each image. cmaps: colormaps for monochrome images.\nWe will visualize line segments from kornia SOLD2\n\ntimg1 = image_to_tensor(cv2.cvtColor(cv2.imread('data/img1.ppm'), cv2.COLOR_BGR2RGB), False).float()/255.\ntimg2 = image_to_tensor(cv2.cvtColor(cv2.imread('data/img3.ppm'), cv2.COLOR_BGR2RGB), False).float()/255.\n\nsold2 = kornia.feature.SOLD2(pretrained=True, config=None)\n\ntimg1_gray = kornia.color.rgb_to_grayscale(timg1)\ntimg2_gray = kornia.color.rgb_to_grayscale(timg2)\n\nwith torch.inference_mode():\n    outputs = sold2(torch.cat([timg1_gray, timg2_gray], dim=0))\n\nline_seg1 = outputs[\"line_segments\"][0]\nline_seg2 = outputs[\"line_segments\"][1]\ndesc1 = outputs[\"dense_desc\"][0]\ndesc2 = outputs[\"dense_desc\"][1]\n\nimgs_to_plot = [tensor_to_image(timg1_gray), tensor_to_image(timg2_gray)]\n\nlines_to_plot = [line_seg1.numpy(), line_seg2.numpy()]\n\nplot_images(imgs_to_plot, [\"Image 1 - detected lines\", \"Image 2 - detected lines\"])\nplot_lines(lines_to_plot, ps=3, lw=2, indices={0, 1})\n\n\nwith torch.inference_mode():\n    matches = sold2.match(line_seg1, line_seg2, desc1[None], desc2[None])\n\nvalid_matches = matches != -1\nmatch_indices = matches[valid_matches]\n\nmatched_lines1 = line_seg1[valid_matches]\nmatched_lines2 = line_seg2[match_indices]\n\nplot_images(imgs_to_plot, [\"Image 1 - matched lines\", \"Image 2 - matched lines\"])\nplot_color_line_matches([matched_lines1, matched_lines2], lw=2)\n\n\n\n\n\n\n\n\nimport cv2\n\nimg_gray = cv2.imread('data/img1.ppm', 0)\n\nlsd = cv2.createLineSegmentDetector(0)\nlines = lsd.detect(img_gray)[0].reshape(-1, 2, 2)\n\nlines = lines[..., ::-1]\n\n# OpenCV LSD detector return xy order\n\nplot_images([img_gray], [\"Image 1 - detected LSD lines\"])\nplot_lines([lines], ps=3, lw=2, indices={0})"
  }
]