# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/wbs.ipynb (unless otherwise specified).

__all__ = ['no_grad', 'extract_descs', 'affnet_zero', 'laf_from_opencv_kpts', 'extract_As', 'n2t', 'affine_match',
           'verify_pygcransac_aff', 'draw_epi_lines', 'draw_affmatches']

# Cell
def no_grad(x):
    with torch.no_grad():
        return x

# Cell
def extract_descs(kpts, img, As = None,
                  bs = 128,
                  descriptor_module = KF.HardNet(True), onGPU=True):
    descriptor_module.eval()
    device = torch.device('cpu')
    if onGPU:
        device = torch.device('cuda:0')
    descriptor_module = descriptor_module.to(device)
    input_data = (kpts,As) if As is not None else kpts
    input_type = 'cv2+A' if As is not None else 'cv2'

    patches = np.array(extract_patches(input_data,
                                       cv2.cvtColor(img, cv2.COLOR_RGB2GRAY),
                                        32, 12., input_type )).astype(np.float32)
    descs = np.zeros((len(patches), 128))
    for i in range(0, len(patches), bs):
        data_a = torch.from_numpy(patches[i:min(i + bs, len(patches)),  :, :]).to(device).float().unsqueeze(1)
        out_a = descriptor_module(data_a)
        descs[i:i + bs] = out_a.cpu().detach().numpy()
    return descs

# Cell
def affnet_zero(x):
    '''Dummy function, which returns upright circle affine shape'''
    b=x.size(0)
    return torch.cat([torch.ones(b,1),
                      torch.zeros(b,1),
                      torch.ones(b,1)],dim=1).to(x.device).float()


# Cell
def laf_from_opencv_kpts(kpts, As = None,  mrSize=6.0, device=torch.device('cpu')):
    N = len(kpts)
    xy = torch.tensor([(x.pt[0], x.pt[1]) for x in kpts ], device=device, dtype=torch.float).view(1, N, 2)
    scales = torch.tensor([(mrSize * x.size) for x in kpts ], device=device, dtype=torch.float).view(1, N, 1, 1)
    angles = torch.tensor([(-x.angle) for x in kpts ], device=device, dtype=torch.float).view(1, N, 1)
    laf = get_laf(xy, scales, angles).view(-1, 2,3)
    if As is not None:
        laf[:, :2,:2] = torch.bmm(laf[:, :2,:2], torch.from_numpy(As).to(device).float())
    return laf.reshape(1,-1,2,3)

# Cell
def extract_As(kpts, img, affnet_module=affnet_zero, bs=128, onGPU=True):
    device = torch.device('cpu')
    if onGPU:
        device = torch.device('cuda:0')
    patches = np.array(extract_patches(kpts,
                                       cv2.cvtColor(img, cv2.COLOR_RGB2GRAY),
                                       32, 12., 'cv2')).astype(np.float32)
    aff = np.zeros((len(patches), 3))
    for i in range(0, len(patches), bs):
        data_a = torch.from_numpy(patches[i:min(i + bs, len(patches)),  :, :]).to(device).unsqueeze(1)
        with torch.no_grad():
            out_a = affnet_module(data_a)
            aff[i:i + bs] = out_a.cpu().detach().numpy()
    aff = torch.from_numpy(aff)
    aff2 = torch.cat([aff[:,0:1], torch.zeros_like(aff[:,0:1]), aff[:,1:2], aff[:,2:3]], dim=1).reshape(-1,2,2)
    laf = torch.cat([aff2,torch.zeros_like(aff2[:,:, 0:1])],dim=2).unsqueeze(1)
    ls = KF.get_laf_scale(laf)
    laf2 = KF.scale_laf(KF.make_upright(laf), 1./ls).squeeze(1)
    return laf2[:,:2,:2].detach().cpu().numpy()

# Cell
def n2t(x): return torch.from_numpy(x).float()

# Cell
def affine_match(img1, img2,
                 aff = affnet_zero,
                det = cv2.xfeatures2d.SIFT_create(8000, contrastThreshold=-10000, edgeThreshold=-10000),
                desc = KF.HardNet(True),
                 onGPU=True,
                 bs=128, th = 0.9):
    device = torch.device('cpu')
    if onGPU:
        device = torch.device('cuda:0')
    kps1 = det.detect(img1,None)
    As1 = extract_As (kps1, img1, affnet_module=aff, bs=bs, onGPU=onGPU)
    descs1 = extract_descs(kps1, img1, As=As1, bs=bs,descriptor_module=desc,onGPU=onGPU)

    kps2 = det.detect(img2,None)
    As2 = extract_As (kps2, img2, affnet_module=aff, bs=bs, onGPU=onGPU)
    descs2 = extract_descs(kps2, img2, As=As2, bs=bs,descriptor_module=desc,onGPU=onGPU)

    tent_vals, tent_idx = KF.match_smnn(n2t(descs1).to(device),
                                        n2t(descs2).to(device), th)
    return {"kp1": kps1,
            "kp2": kps2,
            "As1": As1,
            "As2": As2,
            "descs1": descs1,
            "descs2": descs2,
            "tents": tent_idx.detach().cpu().numpy()}

# Cell
def verify_pygcransac_aff(match_dict_aff, img1, img2):
    h1,w1 = img1.shape[0], img1.shape[1]
    h2,w2 = img2.shape[0], img2.shape[1]
    tents = np.array(match_dict_aff['tents'])
    kps1 = match_dict_aff['kp1']
    kps2 = match_dict_aff['kp2']
    srcPts = np.array([(kps1[x].pt[0],kps1[x].pt[1]) for x in tents[:,0]]).reshape(-1,2)
    dstPts = np.array([(kps2[x].pt[0],kps2[x].pt[1]) for x in tents[:,1]]).reshape(-1,2)
    A1 = laf_from_opencv_kpts([kps1[i] for i in tents[:,0]],
                              match_dict_aff['As1'][tents[:,0]]).squeeze(0)[:,:,:2]
    A2 = laf_from_opencv_kpts([kps2[i] for i in tents[:,1]],
                              match_dict_aff['As2'][tents[:,1]]).squeeze(0)[:,:,:2]
    affines = [np.matmul(a2, np.linalg.inv(a1)) for a1, a2 in zip(A1.detach().numpy(), A2.detach().numpy())]
    affines = np.array(affines).reshape(-1, 4)

    F, mask = pyrobustac.findFundamentalMat(
        np.ascontiguousarray(srcPts.astype(np.float64)),
        np.ascontiguousarray(dstPts.astype(np.float64)),
        np.ascontiguousarray(affines.astype(np.float64)),
        h1, w1, h2, w2, 0.75, max_iters=1000)
    return F, mask

# Cell
def draw_epi_lines(img1,img2,lines,pts1,pts2):
    ''' img1 - image on which we draw the epilines for the points in img2
        lines - corresponding epilines '''
    r,c,ch = img1.shape
    img1o = deepcopy(img1)
    img2o = deepcopy(img2)
    for r,pt1,pt2 in zip(lines,pts1,pts2):
        color = tuple(np.random.randint(0,255,3).tolist())
        x0,y0 = map(int, [0, -r[2]/r[1] ])
        x1,y1 = map(int, [c, -(r[2]+r[0]*c)/r[1] ])
        img1o = cv2.line(img1o, (x0,y0), (x1,y1), color,1)
        img1o = cv2.circle(img1o,tuple(pt1.squeeze().astype(np.int32)),5,color,-1)
        img2o = cv2.circle(img2o,tuple(pt2.squeeze().astype(np.int32)),5,color,-1)
    return img1o,img2o

# Cell
def draw_affmatches(kps1, kps2, As1, As2, tentatives, img1, img2, matchesMask=None,
                    F = None,
                   with_epi_lines= False):
    good = []
    for i in range(len(tentatives)):
        good.append(cv2.DMatch(tentatives[i,0],tentatives[i,1], 1))
    src_pts = np.float32([ kps1[m[0]].pt for m in tentatives ]).reshape(-1,2)
    dst_pts = np.float32([ kps2[m[1]].pt for m in tentatives ]).reshape(-1,2)
    if matchesMask is None:
        matchesMask = [True for x in range(len(tentatives))]
    h,w,ch = img1.shape
    pts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2)
    #dst = cv2.perspectiveTransform(pts, H)
    #Ground truth transformation
    #dst_GT = cv2.perspectiveTransform(pts, H_gt)
    img2_tr = deepcopy(img2)
    #img2_tr = cv2.polylines(decolorize(img2),[np.int32(dst)],True,(0,0,255),3, cv2.LINE_AA)
    #img2_tr = cv2.polylines(deepcopy(img2_tr),[np.int32(dst_GT)],True,(0,255,0),3, cv2.LINE_AA)
    # Blue is estimated, green is ground truth homography
    draw_params = dict(matchColor = (255,255,0), # draw matches in yellow color
                   singlePointColor = None,
                   matchesMask = matchesMask, # draw only inliers
                   flags = cv2.DRAW_MATCHES_FLAGS_DEFAULT+2)
    lafs1 = laf_from_opencv_kpts(kps1, As1 ,3.0)
    lafs2 = laf_from_opencv_kpts(kps2, As2, 3.0)
    x1,y1 = KF.laf.get_laf_pts_to_draw(lafs1[:,tentatives[matchesMask,0]], 0)
    x2,y2 = KF.laf.get_laf_pts_to_draw(lafs2[:,tentatives[matchesMask,1]], 0)
    plt.figure(figsize=(20,16))


    img1_draw = img1
    img2_draw = img2_tr
    if with_epi_lines and (F is not None):
        lines1 = cv2.computeCorrespondEpilines(dst_pts[matchesMask].reshape(-1,1,2), 2, F)
        lines1 = lines1.reshape(-1,3)
        img5gt,img6gt = draw_epi_lines(img1_draw,img2_draw,
                                  lines1,
                                  src_pts[matchesMask],
                                  dst_pts[matchesMask])
        # Find epilines corresponding to points in left image (first image) and
        # drawing its lines on right image
        lines2 = cv2.computeCorrespondEpilines(src_pts[matchesMask].reshape(-1,1,2), 1,F)
        lines2 = lines2.reshape(-1,3)
        img3gt,img4gt = draw_epi_lines(img2_draw,img1_draw,lines2,
                                  dst_pts[matchesMask],
                                  src_pts[matchesMask])
        img1_draw = img3gt
        img2_draw = img5gt
    img_out = cv2.drawMatches(img1_draw,
                              [cv2.KeyPoint(x.pt[0],x.pt[1],x.size, x.angle) for x in kps1],
                              img2_draw,
                              [cv2.KeyPoint(x.pt[0],x.pt[1],x.size, x.angle) for x in kps2],
                              good,None,**draw_params)
    plt.imshow(img_out)
    plt.plot(x1, y1, 'y')
    plt.plot(x2+w, y2, 'y')
    return